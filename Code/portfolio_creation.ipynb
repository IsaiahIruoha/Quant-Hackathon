{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import cvxpy as cp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 1: Load and Merge Data ----\n",
    "\n",
    "# Paths to your data files (update these paths)\n",
    "hackathon_sample_v2_path = \"/Users/isaiah/Desktop/Career/Clubs : Groups/Quant Hackathon/McGill-FIAM Asset Management Hackathon/ml-algorithmic-trading/Data/hackathon_sample_v2.csv\"\n",
    "output_path = \"/Users/isaiah/Desktop/Career/Clubs : Groups/Quant Hackathon/McGill-FIAM Asset Management Hackathon/ml-algorithmic-trading/Data/full_dataset_predictions.csv\"\n",
    "mkt_path = \"/Users/isaiah/Desktop/Career/Clubs : Groups/Quant Hackathon/McGill-FIAM Asset Management Hackathon/ml-algorithmic-trading/Data/mkt_ind.csv\"\n",
    "\n",
    "# Load data from hackathon_sample_v2.csv\n",
    "hackathon_sample_v2 = pd.read_csv(\n",
    "    hackathon_sample_v2_path,\n",
    "    usecols=[\n",
    "        'permno', 'date', 'market_equity', 'be_me', 'ret_12_1',\n",
    "        'ivol_capm_21d', 'stock_exret'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load predictions from output.csv\n",
    "output = pd.read_csv(output_path, usecols=['permno', 'date', 'Predicted_Excess_Return'])\n",
    "\n",
    "# Convert 'date' columns to datetime format\n",
    "hackathon_sample_v2['date'] = pd.to_datetime(hackathon_sample_v2['date'], format='%Y%m%d')  # Ensure correct date format\n",
    "output['date'] = pd.to_datetime(output['date'])  # Already in datetime, but this ensures uniformity\n",
    "\n",
    "# Merge datasets on 'permno' and 'date'\n",
    "pred = pd.merge(hackathon_sample_v2, output, on=['permno', 'date'], how='inner')\n",
    "\n",
    "# Check merged data\n",
    "print(pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 2: Data Cleaning ----\n",
    "\n",
    "# Drop rows with any null values\n",
    "pred = pred.dropna(axis=0).reset_index(drop=True)\n",
    "\n",
    "# Ensure 'stock_exret' is numeric\n",
    "pred['stock_exret'] = pd.to_numeric(pred['stock_exret'], errors='coerce')\n",
    "pred = pred.dropna(subset=['stock_exret'])\n",
    "\n",
    "# Extract 'year' and 'month' from 'date'\n",
    "pred['year'] = pred['date'].dt.year\n",
    "pred['month'] = pred['date'].dt.month\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 3: Multi-Signal Ensemble with Random Forest ----\n",
    "\n",
    "# Create additional signals\n",
    "pred['value_signal'] = pred['be_me']                # Value factor\n",
    "pred['momentum_signal'] = pred['ret_12_1']          # Momentum factor\n",
    "pred['risk_signal'] = 1 / pred['ivol_capm_21d']     # Inverse volatility\n",
    "pred['Predicted_Excess_Return_signal'] = pred['Predicted_Excess_Return']                    # ElasticNet predictions\n",
    "\n",
    "# Create target variable\n",
    "pred['stock_exret_next'] = pred.groupby('permno')['stock_exret'].shift(-1)\n",
    "\n",
    "# Drop rows where 'stock_exret_next' is NaN (these would be the last months for each stock)\n",
    "pred = pred.dropna(subset=['stock_exret_next'])\n",
    "\n",
    "# Features and target\n",
    "features = pred[['value_signal', 'momentum_signal', 'risk_signal', 'Predicted_Excess_Return_signal']]\n",
    "target = pred['stock_exret_next']\n",
    "\n",
    "# Sort data by date\n",
    "pred = pred.sort_values(by='date')\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best estimator from hyperparameter tuning\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Fit the model with the best parameters on the training data\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "\n",
    "# Define test_data based on the test set indices\n",
    "test_data = pred.loc[X_test.index].copy()\n",
    "\n",
    "# Predict on the test data\n",
    "test_data['predicted_return'] = best_rf.predict(X_test_scaled)\n",
    "\n",
    "# Add predicted_return back to the full dataset\n",
    "pred.loc[test_data.index, 'predicted_return'] = test_data['predicted_return']\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print the best parameters and performance metrics\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# ---- Verify 'predicted_return' Exists ----\n",
    "print(\"\\nColumns in 'pred' DataFrame:\", pred.columns.tolist())\n",
    "print(\"'predicted_return' Missing Values:\", pred['predicted_return'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 4: Monthly Rebalancing with Black-Litterman Optimization ----\n",
    "\n",
    "# Try using on portfolio's rather then individual stocks, SML, HML, etc.\n",
    "\n",
    "# Check that market cap is not forward looking\n",
    "# pred['be_me'] = pred['be_me'].shift(-1)\n",
    "\n",
    "# Initialize list to store monthly portfolio data\n",
    "monthly_results = []\n",
    "\n",
    "# Get list of unique months\n",
    "pred['month_period'] = pred['date'].dt.to_period('M')\n",
    "unique_months = pred['month_period'].unique()\n",
    "\n",
    "for current_month in unique_months:\n",
    "    # Filter data for the current month\n",
    "    monthly_data = pred[pred['month_period'] == current_month].copy()\n",
    "    \n",
    "    if monthly_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calculate market weights\n",
    "    monthly_data['market_weight'] = (\n",
    "        monthly_data['market_equity'] / monthly_data['market_equity'].sum()\n",
    "    )\n",
    "    \n",
    "    # Calculate market equilibrium return\n",
    "    market_equilibrium_return = np.dot(\n",
    "        monthly_data['market_weight'], monthly_data['stock_exret']\n",
    "    )\n",
    "    \n",
    "    # Black-Litterman adjusted returns\n",
    "    tau = 0.05  # Confidence in views\n",
    "    monthly_data['bl_adjusted_returns'] = (\n",
    "        (1 - tau) * market_equilibrium_return + tau * monthly_data['predicted_return']\n",
    "    )\n",
    "    \n",
    "    # Create covariance matrix using rolling window (e.g., past 12 months)\n",
    "    start_date = (current_month - 11).to_timestamp()\n",
    "    end_date = current_month.to_timestamp()\n",
    "    window_data = pred[\n",
    "        (pred['date'] >= start_date) & (pred['date'] <= end_date)\n",
    "    ]\n",
    "    \n",
    "    # Pivot returns for covariance calculation\n",
    "    pivoted_returns = window_data.pivot(\n",
    "        index='date', columns='permno', values='stock_exret'\n",
    "    )\n",
    "    \n",
    "    # Drop assets with missing data\n",
    "    pivoted_returns = pivoted_returns.dropna(axis=1, how='any')\n",
    "    \n",
    "    # Check if there is enough data\n",
    "    if pivoted_returns.shape[0] < 2 or pivoted_returns.shape[1] == 0:\n",
    "        print(f\"Not enough data for covariance matrix in {current_month}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix_df = pivoted_returns.cov()\n",
    "    cov_matrix_df = (cov_matrix_df + cov_matrix_df.T) / 2  # Ensure symmetry\n",
    "    \n",
    "    # Align assets in monthly_data with covariance matrix\n",
    "    common_permnos = monthly_data['permno'].isin(cov_matrix_df.columns)\n",
    "    monthly_data = monthly_data[common_permnos]\n",
    "    \n",
    "    if monthly_data.empty:\n",
    "        print(f\"No matching assets in {current_month}\")\n",
    "        continue\n",
    "    \n",
    "    monthly_data.set_index('permno', inplace=True)\n",
    "    monthly_data = monthly_data.loc[cov_matrix_df.columns.intersection(monthly_data.index)]\n",
    "    \n",
    "    # Extract adjusted returns and covariance matrix\n",
    "    bl_adjusted_returns = monthly_data['bl_adjusted_returns'].values\n",
    "    cov_matrix = cov_matrix_df.loc[monthly_data.index, monthly_data.index].values\n",
    "    \n",
    "    # Ensure covariance matrix is positive semidefinite\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)\n",
    "    if np.any(eigenvalues < 0):\n",
    "        epsilon = 1e-6\n",
    "        cov_matrix += epsilon * np.eye(cov_matrix.shape[0])\n",
    "    \n",
    "    # Wrap covariance matrix as CVXPY constant\n",
    "    cov_matrix_cvx = cp.Constant(cov_matrix)\n",
    "    \n",
    "    # Define optimization variables\n",
    "    n_assets = len(bl_adjusted_returns)\n",
    "    weights = cp.Variable(n_assets)\n",
    "    \n",
    "    # Define objective function and constraints\n",
    "    delta = 2.5  # Risk aversion parameter\n",
    "    portfolio_return = bl_adjusted_returns @ weights\n",
    "    portfolio_risk = cp.quad_form(weights, cov_matrix_cvx)\n",
    "    objective = cp.Maximize(portfolio_return - (delta / 2) * portfolio_risk)\n",
    "    constraints = [\n",
    "        cp.sum(weights) == 1,\n",
    "        weights >= -0.05,  # Allow short selling with a max short position of 5%\n",
    "        weights <= 0.05    # Max long position is 5%\n",
    "    ]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    \n",
    "    if problem.status not in [\"infeasible\", \"unbounded\"]:\n",
    "        # Assign optimized weights\n",
    "        monthly_data['optimal_weight'] = weights.value\n",
    "        monthly_data.reset_index(inplace=True)\n",
    "        monthly_results.append(monthly_data)\n",
    "    else:\n",
    "        print(f\"Optimization failed for {current_month}\")\n",
    "        continue\n",
    "\n",
    "# Combine monthly data\n",
    "if monthly_results:\n",
    "    final_portfolio = pd.concat(monthly_results, ignore_index=True)\n",
    "else:\n",
    "    print(\"No monthly results to combine.\")\n",
    "    final_portfolio = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 5: Construct Long-Short Portfolio ----\n",
    "\n",
    "# Select top and bottom N stocks based on optimized weights\n",
    "N = 50  # Number of stocks for long and short positions\n",
    "final_portfolio = final_portfolio.sort_values(\n",
    "    by=['date', 'optimal_weight'], ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Assign positions and weights for each month\n",
    "portfolio_list = []\n",
    "\n",
    "for date, group in final_portfolio.groupby('date'):\n",
    "    group = group.copy()\n",
    "    group = group.sort_values(by='optimal_weight', ascending=False)\n",
    "    \n",
    "    long_positions = group.head(N)\n",
    "    short_positions = group.tail(N)\n",
    "    \n",
    "    long_positions['position'] = 'long'\n",
    "    short_positions['position'] = 'short'\n",
    "    \n",
    "    # Assign weights\n",
    "    long_positions['weight'] = long_positions['optimal_weight'] / long_positions['optimal_weight'].sum()\n",
    "    short_positions['weight'] = -short_positions['optimal_weight'] / short_positions['optimal_weight'].sum()\n",
    "    \n",
    "    # Combine positions\n",
    "    combined = pd.concat([long_positions, short_positions])\n",
    "    portfolio_list.append(combined)\n",
    "\n",
    "# Combine all positions\n",
    "final_portfolio = pd.concat(portfolio_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 6: Calculate Portfolio Performance Metrics ----\n",
    "\n",
    "# Calculate weighted returns\n",
    "final_portfolio['weighted_return'] = final_portfolio['stock_exret'] * final_portfolio['weight']\n",
    "\n",
    "# Calculate daily portfolio returns\n",
    "portfolio_returns = final_portfolio.groupby('date')['weighted_return'].sum()\n",
    "\n",
    "# Calculate cumulative returns\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "# Performance metrics\n",
    "# Assuming portfolio_returns is monthly\n",
    "mean_return = portfolio_returns.mean()\n",
    "std_return = portfolio_returns.std()\n",
    "\n",
    "# Annualize return and volatility for the Sharpe ratio\n",
    "annualized_return = mean_return * 12\n",
    "annualized_volatility = std_return * np.sqrt(12)\n",
    "sharpe_ratio = annualized_return / annualized_volatility\n",
    "print(\"Sharpe Ratio:\", sharpe_ratio)\n",
    "\n",
    "annualized_return = mean_return * 12\n",
    "print(\"Annualized Return:\", annualized_return)\n",
    "\n",
    "annualized_std_dev = std_return * np.sqrt(12)\n",
    "print(\"Annualized Standard Deviation:\", annualized_std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 7: Calculate CAPM Alpha and Information Ratio ----\n",
    "\n",
    "# Load market data\n",
    "mkt = pd.read_csv(mkt_path)\n",
    "\n",
    "# Create 'mkt_rf' by subtracting risk-free rate\n",
    "mkt['mkt_rf'] = mkt['sp_ret'] - mkt['rf']\n",
    "\n",
    "# Prepare portfolio returns data (make sure 'date' exists in portfolio_returns)\n",
    "portfolio_returns = portfolio_returns.reset_index()\n",
    "\n",
    "# Create 'year' and 'month' columns in portfolio_returns (if they don't exist already)\n",
    "portfolio_returns['year'] = portfolio_returns['date'].dt.year\n",
    "portfolio_returns['month'] = portfolio_returns['date'].dt.month\n",
    "\n",
    "# Merge portfolio and market data on 'year' and 'month'\n",
    "regression_data = pd.merge(\n",
    "    portfolio_returns, mkt[['year', 'month', 'mkt_rf']], on=['year', 'month'], how='inner'\n",
    ")\n",
    "\n",
    "# Perform CAPM regression\n",
    "nw_ols = sm.ols(formula=\"weighted_return ~ mkt_rf\", data=regression_data).fit(\n",
    "    cov_type=\"HAC\", cov_kwds={\"maxlags\": 3}, use_t=True\n",
    ")\n",
    "print(nw_ols.summary())\n",
    "\n",
    "# Extract metrics\n",
    "alpha = nw_ols.params['Intercept']\n",
    "t_stat = nw_ols.tvalues['Intercept']\n",
    "info_ratio = alpha / np.sqrt(nw_ols.mse_resid) * np.sqrt(12)\n",
    "\n",
    "print(\"CAPM Alpha:\", alpha)\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"Information Ratio:\", info_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 8: Calculate Maximum Drawdown ----\n",
    "\n",
    "# Maximum Drawdown Calculation\n",
    "if not cumulative_returns.empty:\n",
    "    rolling_max = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "else:\n",
    "    max_drawdown = 0  # No drawdown in empty or zero-return periods\n",
    "    \n",
    "print(\"Maximum Drawdown:\", max_drawdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 9: Calculate Turnover ----\n",
    "\n",
    "def calculate_turnover(portfolio):\n",
    "    portfolio['month'] = portfolio['date'].dt.to_period('M')\n",
    "    turnover_list = []\n",
    "    months = sorted(portfolio['month'].unique())\n",
    "\n",
    "    for i in range(1, len(months)):\n",
    "        current_month = months[i]\n",
    "        previous_month = months[i - 1]\n",
    "\n",
    "        current_weights = portfolio[portfolio['month'] == current_month].set_index('permno')['optimal_weight']\n",
    "        previous_weights = portfolio[portfolio['month'] == previous_month].set_index('permno')['optimal_weight']\n",
    "        \n",
    "        # Align the two weight vectors\n",
    "        all_permnos = current_weights.index.union(previous_weights.index)\n",
    "        current_weights = current_weights.reindex(all_permnos, fill_value=0)\n",
    "        previous_weights = previous_weights.reindex(all_permnos, fill_value=0)\n",
    "\n",
    "        # Turnover is the sum of the absolute changes in weights\n",
    "        turnover = (current_weights - previous_weights).abs().sum() / 2\n",
    "        turnover_list.append(turnover)\n",
    "\n",
    "    average_turnover = np.mean(turnover_list)\n",
    "    return average_turnover\n",
    "\n",
    "long_portfolio = final_portfolio[final_portfolio['position'] == 'long']\n",
    "short_portfolio = final_portfolio[final_portfolio['position'] == 'short']\n",
    "\n",
    "long_turnover = calculate_turnover(long_portfolio)\n",
    "short_turnover = calculate_turnover(short_portfolio)\n",
    "print(\"Long Portfolio Turnover:\", long_turnover)\n",
    "print(\"Short Portfolio Turnover:\", short_turnover)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEC292",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
